if(confidence.level < 0 || confidence.level > 1) stop("Confidence level must be in the range [0,1]")
mean <- mean(y)
mean.sd <- 1/sqrt(length(y))
y1 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd)
y2 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd, lower.tail = FALSE)
return(c(y1, y2))
}
# This is a function that generates data that is in form y = y0 + epsilon where the
# error epsilon has normal distribution N(0, 1) as assumed by the first algorithm.
GenerateData <- function(y0, n)
{
return(y0 + rnorm(n, mean = 0, sd = 1))
}
# Test that the first algorithm works correctly, i.e., on average the algorithm manages
# to output such intervals that the fraction when the algorithm is correct is roughly
# around confidence level for large enough number of samples.
# Illustrative example with illusrative plot
# Test parameters
y0 <- 0
n <- 200
k <- 100
confidence.level <- 0.5
# Plot setup
plot(c(), c(), ylab = "Runs", xlab = "Guesses",  xlim = c(y0 - 3, y0 + 3), ylim = c(0, 1), axes = FALSE)
xlabels <- c(expression(y[0] - 3), expression(y[0] - 2), expression(y[0] - 1), expression(y[0]))
xlabels <- c(xlabels, expression(y[0] + 1), expression(y[0] + 2), expression(y[0] + 3))
Axis(side=1, at = c(-3:3), labels = xlabels)
lines(c(y0, y0), c(0,1), lwd = 4, lty = "solid")
# Test runs
success <- c(NA, k)
for(i in c(1:k))
{
y <- GenerateData(y0, n)
interval <- GetConfidenceInterval(y, confidence.level)
success[i] <- (interval[1] <= y0) && (y0 <= interval[2])
lines(interval, c(i/k, i/k), col = c("red", "blue")[1 +  success[i]])
}
title(sprintf("Set 3 The correct value appeared in the interval in  %2.0f%% of runs", mean(success)*100))
y0 <- 0
n <- 200
k <- 100
confidence.level <- 0.5
# Plot setup
plot(c(), c(), ylab = "Runs", xlab = "Guesses",  xlim = c(y0 - 3, y0 + 3), ylim = c(0, 1), axes = FALSE)
xlabels <- c(expression(y[0] - 3), expression(y[0] - 2), expression(y[0] - 1), expression(y[0]))
xlabels <- c(xlabels, expression(y[0] + 1), expression(y[0] + 2), expression(y[0] + 3))
Axis(side=1, at = c(-3:3), labels = xlabels)
lines(c(y0, y0), c(0,1), lwd = 4, lty = "solid")
# Test runs
success <- c(NA, k)
for(i in c(1:k))
{
y <- GenerateData(y0, n)
interval <- GetConfidenceInterval(y, confidence.level)
success[i] <- (interval[1] <= y0) && (y0 <= interval[2])
lines(interval, c(i/k, i/k), col = c("red", "blue")[1 +  success[i]])
}
title(sprintf("Set 3 The correct value appeared in the interval in  %2.0f%% of runs", mean(success)*100))
n <- c()
cost <- c()
success <- c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
interval <- GetConfidenceInterval(y, 0.50)
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
barplot(cost - 0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of high-quality scan", xlab = "Run")
barplot(cost, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Overall cost", ylim = c(0,10), xlab = "Run")
par(mfrow = c(1, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
barplot(cost - 0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of high-quality scan", xlab = "Run")
barplot(cost, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Overall cost", ylim = c(0,10), xlab = "Run")
n <- c()
cost <- c()
success <- c()
rm(list=ls())
n <- c()
cost <- c()
success <- c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
interval <- GetConfidenceInterval(y, 0.50)
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
GetConfidenceInterval <- function(y, confidence.level)
{
if(length(y) == 1) stop("At least two observations are needed")
if(confidence.level < 0 || confidence.level > 1) stop("Confidence level must be in the range [0,1]")
mean <- mean(y)
mean.sd <- 1/sqrt(length(y))
y1 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd)
y2 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd, lower.tail = FALSE)
return(c(y1, y2))
}
GetConfidenceInterval(c(1,3,5), 0.95)
rm(list=ls())
# This is a procedure that given a list of observations that are assumed to come
# from a model y = y0 + epsilon where the error epsilon is assumed to have normal
# distribution N(0, 1) outputs a symmetric confidence interval.
GetConfidenceInterval <- function(y, confidence.level)
{
if(length(y) == 1) stop("At least two observations are needed")
if(confidence.level < 0 || confidence.level > 1) stop("Confidence level must be in the range [0,1]")
mean <- mean(y)
mean.sd <- 1/sqrt(length(y))
y1 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd)
y2 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd, lower.tail = FALSE)
return(c(y1, y2))
}
# This is a function that generates data that is in form y = y0 + epsilon where the
# error epsilon has normal distribution N(0, 1) as assumed by the first algorithm.
GenerateData <- function(y0, n)
{
return(y0 + rnorm(n, mean = 0, sd = 1))
}
n <- c()
cost <- c()
success <- c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
interval <- GetConfidenceInterval(y, 0.50)
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
summary(success)
summary(success)
success[1]
success[1,1]
dim(success)
length(success)
success
n <- c()
cost <- c()
success <- c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
interval <- GetConfidenceInterval(y, 0.50)
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
barplot(cost - 0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of high-quality scan", xlab = "Run")
barplot(cost, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Overall cost", ylim = c(0,10), xlab = "Run")
par(mfrow = c(1, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
success[1]
cost[1]
success[20]
success[21]
success[22]
success[20]
cost[20]
y0
rm(list=ls())
setwd("/home/aqeel/Study/ML/HW03\ (Same\ Practice\ Session\ 02)")
load("exercise-1.Rdata")
#### First Question A #####
msecalculator<-function(mv)
{
ypr<-data$x*mv[,2]+mv[,1]
mse.matrix <- 1/100 * t(data$y-ypr) %*%(data$y-ypr)
return(mse.matrix)
}
m<-expand.grid(x=seq(0,2,0.1),y=seq(-5,5,0.1))
mse<-NULL
for (i in 1:nrow(m))
{
mse<-rbind(mse,msecalculator(m[i,]))
}
library(plot3D)
plot3D::polygon3D(m$x,m$y,mse)
mse[which.min(mse)]
m[which.min(mse),]
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
rm(list=ls())
# This is a procedure that given a list of observations that are assumed to come
# from a model y = y0 + epsilon where the error epsilon is assumed to have normal
# distribution N(0, 1) outputs a symmetric confidence interval.
GetConfidenceInterval <- function(y, confidence.level)
{
if(length(y) == 1) stop("At least two observations are needed")
if(confidence.level < 0 || confidence.level > 1) stop("Confidence level must be in the range [0,1]")
mean <- mean(y)
mean.sd <- 1/sqrt(length(y))
y1 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd)
y2 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd, lower.tail = FALSE)
return(c(y1, y2))
}
GenerateData <- function(y0, n)
{
return(y0 + rnorm(n, mean = 0, sd = 1))
}
n <- c()
cost <- c()
success <- c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
interval <- GetConfidenceInterval(y, 0.50)
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
barplot(cost - 0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of high-quality scan", xlab = "Run")
barplot(cost, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Overall cost", ylim = c(0,10), xlab = "Run")
par(mfrow = c(1, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
?sample
precision[k]<-sample(c(0.1,0.9,0.1))
precision<-c()
sample(c(0.1,0.9,0.1))
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1,0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
sample(c(0.1:0.9,0.1),1)
c(0.1:0.9)
c(0.1:0.9)
c(0.1:0.9)
c(0.1:0.9)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
sample(seq(0.1,0.9,0.1),1)
n <- c()
cost <- c()
success <- c()
precision<-c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
?sample
precision[k]<-sample(seq(0.1,0.9,0.1),1)
interval <- GetConfidenceInterval(y, precision[k])
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
barplot(precision, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(precision, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 1),  xlab = "Run")
barplot(precision, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 1),  xlab = "Run")
barplot(precision, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Confidience Level", ylim=c(0, 1),  xlab = "Run")
par(mfrow = c(2, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
barplot(0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of low-quality measurements", ylim=c(0, 5),  xlab = "Run")
barplot(cost - 0.05 * n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Cost of high-quality scan", xlab = "Run")
barplot(cost, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Overall cost", ylim = c(0,10), xlab = "Run")
par(mfrow = c(1, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements", ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements",xlim = c(0,20), ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
par(mfrow = c(1, 2))
barplot(n, space = 0.1, col = c("red", "blue")[1 + success], ylab = "Number of measurements",xlim = c(0,20), ylim=c(0, 100),  xlab = "Run")
plot(cumsum(cost)/cumsum(success), type = "s", xlab = "Run", ylab = "Average cost")
rm(list=ls())
# This is a procedure that given a list of observations that are assumed to come
# from a model y = y0 + epsilon where the error epsilon is assumed to have normal
# distribution N(0, 1) outputs a symmetric confidence interval.
GetConfidenceInterval <- function(y, confidence.level)
{
if(length(y) == 1) stop("At least two observations are needed")
if(confidence.level < 0 || confidence.level > 1) stop("Confidence level must be in the range [0,1]")
mean <- mean(y)
mean.sd <- 1/sqrt(length(y))
y1 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd)
y2 <- qnorm((1 - confidence.level)/2, mean = mean, sd = mean.sd, lower.tail = FALSE)
return(c(y1, y2))
}
# This is a function that generates data that is in form y = y0 + epsilon where the
# error epsilon has normal distribution N(0, 1) as assumed by the first algorithm.
GenerateData <- function(y0, n)
{
return(y0 + rnorm(n, mean = 0, sd = 1))
}
# Test that the first algorithm works correctly, i.e., on average the algorithm manages
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
if (k!=1)
{
}
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
?sample
precision[k]<-sample(seq(0.1,0.9,0.1),1)
interval <- GetConfidenceInterval(y, precision[k])
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
n <- c()
cost <- c()
success <- c()
precision<-c()
for(k in c(1:20)){
y0 <- runif(1, min = -100, max = 100)
if (k!=1)
{
}
# We use a measurements strategy where we make randomly 2, ..., 100 low precision measurements
n[k] <- sample(c(2:100), 1)
y <- GenerateData(y0, n[k])
cost[k] <- 0.05 * n[k]
# Lets estimate the interval for confidence level 50%
?sample
precision[k]<-sample(seq(0.1,0.9,0.1),1)
interval <- GetConfidenceInterval(y, precision[k])
cost[k] <- cost[k] + 1 * (interval[2] - interval[1])
# Lets see whether the high precision scan was successful or not
success[k] <- (interval[1] <= y0) && (y0 <= interval[2])
}
rm(list=ls())
load("exercise-3.Rdata")
# A quick example that shows the most important things in multivariate regression
# Lets train the model
model <- lm(prestige ~ income + education + 1, data = Duncan)
# Observe basic properties of the model
summary(model)
# Export the model coeffients to a usable form
coef(model)
# Extract confidence intervals for model parameters
confint(model)
confint(model, "(Intercept)", level = 0.7)
# Export prediction errors (residuals) to a usable form
residuals(model)
# Lets see how big is the prediction error in precentages for each occupation
plot(residuals(model)/Duncan$prestige*100)
# Lets compute mean square error
mean(residuals(model)^2)
# Lets compute normalised mean square error
base.model <- lm(prestige ~ 1, data = Duncan)
mean(residuals(model)^2)/mean(residuals(base.model)^2)*100
# Shorter version
mean(residuals(model)^2)/mean((Duncan$prestige - mean(Duncan$prestige))^2) * 100
# Even more shorter version
mean(residuals(model)^2)/var(Duncan$prestige)*100
# Polynomial regression
# Lets look at the data
plot(data, pch=16)
# Lets try the linear regression
model <- lm(y ~ x + 1, data = data)
plot(data, pch = 16)
abline(a = coef(model)[1], b = coef(model)[2], col = "red", lwd=2)
# Let prepare the extended data matrix for fitting quadratic relations y ~ x^2 + x + 1
quadratic.data <- data.frame(x1 = data$x, x2 = data$x^2, y = data$y)
plot(data$x, predict(lm(y~x1+x2+1, quadratic.data), newdata = quadratic.data), type="l", col="red")
points(data, pch=16)
deg10.data <- data.frame(x1 = data$x, x2 = data$x^2,x3=data$x^3,x4=data$x^4,x5=data$x^5,
x6=data$x^6,x7=data$x^7,x8=data$x^8,x9=data$x^9,x10=data$x^10, y = data$y)
plot(data$x, predict(lm(y~x1+x2+1, quadratic.data), newdata = quadratic.data), type="l", col="red")
points(data, pch=16)
deg10.data <- data.frame(x1 = data$x, x2 = data$x^2,x3=data$x^3,x4=data$x^4,x5=data$x^5,
x6=data$x^6,x7=data$x^7,x8=data$x^8,x9=data$x^9,x10=data$x^10, y = data$y)
plot(data$x, predict(lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+1, deg10.data ), newdata = quadratic.data), type="l", col="red")
View(deg10.data)
deg10.data <- data.frame(x1 = data$x, x2 = data$x^2,x3=data$x^3,x4=data$x^4,x5=data$x^5,
x6=data$x^6,x7=data$x^7,x8=data$x^8,x9=data$x^9,x10=data$x^10, y = data$y)
plot(data$x, predict(lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+1, deg10.data ), newdata = deg10.data), type="l", col="red")
points(data, pch=16)
coef(lm(y~x1+x2+1, quadratic.data))
coef(lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+1, deg10.data))
coef(lm(y~.,deg10.data))
coef(lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+1, deg10.data))
coef(lm(y~.,deg10.data))
coef(lm(y~poly(x,10,raw=TRUE),data = data))
coef(lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+1, deg10.data))
coef(lm(y~.,data = deg10.data))
coef(lm(y~poly(x,10,raw=TRUE),data = data))
plot(weird.data, pch = 16)
View(weird.data)
